\documentclass[12pt]{article}
\usepackage[top=2cm,bottom=2.5cm,left=2cm,right=2cm,marginparwidth=2cm]{geometry}
%% Language and font encodings
\usepackage[english]{babel}
% \usepackage[utf8x]{inputenc}
\usepackage{listings}
\usepackage{minted}
\usepackage[shortlabels]{enumitem}

%% Sets page size and margins

\usepackage{float}
%% Useful packages
% \usepackage{amsmath}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{listings}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{ {./images/} }
% \DeclareGraphicsExtensions{.pdf,.jpg,.png}

\newtheoremstyle{quest}{\topsep}{\topsep}{}{}{\bfseries}{}{ }{\thmname{#1}\thmnote{ #3}.}
\theoremstyle{quest}
\newtheorem*{problem}{Problem}
\newenvironment{solution}
  {\begin{proof}[Solution]}
  {\end{proof}}
  
 \DeclareMathOperator{\diag}{diag}


%% defined colors
\definecolor{Blue}{rgb}{0,0,0.5}
\definecolor{Green}{rgb}{0,0.75,0.0}
\definecolor{LightGray}{rgb}{0.6,0.6,0.6}
\definecolor{DarkGray}{rgb}{0.3,0.3,0.3}

\title{CMPSC 448: Machine Learning and Algorithmic AI \\ Homework 5}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Instruction}
This HW includes both theory and implementation problems. Please note,
\begin{itemize}
    \item Your code must work with Python 3.7+
    \item You need to submit a report in PDF including all written deliverable, and all implementation codes so one could regenerate your results.
    \item For programming part of PCA problem, you can import any module you would like from \textsf{scikit-learn} or other external libraries to minimize the amount of implementation required to get the coding problem done. Submit your code in \textsf{Problem.py}.
\end{itemize}

\section*{PCA}
\begin{problem}[1] [30 points]
In this assignment, you will explore PCA as a technique for discerning whether low-dimensional structure exists in a set of data and for finding good representations of the data in that subspace. To this end, you will do PCA on a Iris dataset which can be loaded in \textsf{scikit-learn} using following commands:
\end{problem}

\begin{minted}{python}
from sklearn.datasets import load_iris iris = load_iris()
X = iris.data
y = iris.target
\end{minted}

\begin{enumerate}
    \item Carry out a principal component analysis on the entire \textbf{raw} dataset (4-dimensional instances) for $k = 1, 2, 3, 4$ components. How much of variance in data is retained when $k=1,2,3,4$, respectively?
    \item Apply the centering and standardization operations from Problem 1 on raw data set and repeat part (1) on processed data. Explain any differences you observe compared to part (1) and justify.
    \item Project the raw four dimensional data down to a two dimensional subspace generated by first two top principle components (PCs) from part (2) and produce a scatter plot of the data. Make sure you plot each of the three classes differently (using color or different markers). Can you see the three Iris flower clusters?
    \item Either use your k-means++ implementation from previous homework or from \textsf{scikit-learn} to cluster data from part (3) into three clusters. Explain your observations.
\end{enumerate}

\section*{Matrix Factorization}
\begin{problem}[2] [30 points]
Recall the following objective for extracting latent features from a partially observed rating matrix via matrix factorization (MF) for making recommendations, discussed in the class:
\begin{equation}
\label{eq:matrix_fac}
  \min_{\bm{U},\bm{V}}\left[f(\bm{U},\bm{V}) \equiv \sum_{(i,j)\in\Omega}(r_{i,j}-\bm{u}_i^{\top}\bm{v}_j)^2 + \alpha\sum_{i=1}^{n}\lVert\bm{u}_i\rVert_2^2 + \beta\sum_{j=1}^{m}\lVert\bm{v}_j\rVert_2^2 \right]  
\end{equation}
where
\begin{itemize}
    \item $n$: number of users 
    \item $m$: number of items
    \item $r_{i,j}$: $(i,j)$-th element in $\bm{R} \in \mathbb{R}^{n \times m}$ input partially observed rating matrix
    \item $\Omega \subseteq [n]\times[m]$: index of observed entries in rating matrix, where $[n]$ denotes the sequence of numbers $\{1,2,\dots,n\}$.
    \item $k$: number of latent features
    \item $\bm{U} \in \mathbb{R}^{n \times k}$: the (unknown) matrix of latent feature vectors for $n$ users (the $i$th row $\bm{u}_i \in \mathbb{R}^{k}$ is the latent features for $i$th user)
    \item $\bm{V} \in \mathbb{R}^{m \times k}$: the (unknown) matrix of latent feature vectors for $m$ items (the $j$th row $\bm{v}_j \in \mathbb{R}^{k}$ is the latent features for $j$th movie)
\end{itemize}

Please do the followings:
\begin{enumerate}
    \item In solving Equation~\ref{eq:matrix_fac} with iterative Alternating Minimization algorithm (fixing $\bm{V}^{(t)}$ and taking gradient step for $\bm{U}^{(t)}$ and vice versa), discuss what happens if $\bm{U}^{(0)}$ and $\bm{V}^{(0)}$ are initialized to zero?
    \item Discuss why when there is no regularization in basic MF formulated in Equation~\ref{eq:matrix_fac}, i.e., $\alpha = \beta = 0$, each user must have rated at least $k$ movies, and each movie must have been rated by at least $k$ users (Hint: consider the closed form solution for $\bm{u}_i$ and $\bm{v}_j$ in notes and argue why these conditions are necessary without regularization).
    \item Computing the closed form solution in part (2) could be computational burden for large number of users or movies. A remedy for this would be using iterative optimization algorithms such as Stochastic Gradient Descent (SGD) where you sample movies in updating the latent features for users and sample users in updating the latent features of movies. Derive the updating rules for $\bm{u}_i^{(t)}$ and $\bm{v}_j^{(t)}$ at $t$th iteration of SGD. Show the detailed steps.
\end{enumerate}
\end{problem}

\newpage
\section*{MDP and RL}

\begin{problem}[3] [40 points]
Consider the MDP in Figure~\ref{fig:mdp_rl}. There are five states \{A, B, C, D, T\} and two actions $\{a_1,a_2\}$. The state T is the terminal state. The numbers on each arrow show the probability of moving to the next state and the reward, respectively. For example, if the agent takes action $a_1$ at state A, with probability 0.7 it will move to state B  and will be rewarded -10, and with probability 0.3 it will move to state C and will be rewarded -5.
\\\\
\begin{figure}[h!]
\centering
  \includegraphics[width=.9\textwidth]{mdp_example.png}
  \caption{MDP}
  \label{fig:mdp_rl}
\end{figure}
\noindent
\\\\
(1) [10 points] For a policy $\pi$ that always takes action $a_1$ at every state, write down the Bellman Equation for state value function for each state using discount factor $\gamma$.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
(2) [10 points] From the Bellman Equation in (1), compute the state values using \underline{$\gamma=0.5$}.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
(3) [5 points] Consider a random policy $\pi_0$ that uniformly selects actions at each state (i.e., the probability of taking each of two actions under this policy is 0.5). Initialize $v_0(s)=0$ for all states, apply one sweep of policy evaluation to show $v_1(s)$ for all states using \underline{$\gamma=1$}.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
(4) [5 points] Based on the result of (3), apply policy improvement by greedification with \underline{$\gamma=1$} to show the improved policy $\pi_1$. Select actions uniformly if there is a tie.


\newpage
\begin{figure}[h!]
\centering
  \includegraphics[width=.9\textwidth]{mdp_episode.png}
  \caption{An Episode for MDP in Figure~\ref{fig:mdp_rl}.}
  \label{fig:mdp_episode}
\end{figure}
\noindent
Now let's do Temporal Difference Control on this MDP using SARSA and Q-Learning. We initialize $Q(s,a)=0$ for all pairs of $(s,a)$, and we use step size \underline{$\alpha=0.1$} and discount factor \underline{$\gamma=0.5$}. Suppose we sample an episode in Figure~\ref{fig:mdp_episode}.\\\\
(5) [5 points] Use SARSA to update Q values by processing the episode from beginning to end.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
(6) [5 points] Use Q-Learning to update Q values by processing the episode from beginning to end.
\end{problem}


\newpage
\section*{Bonus}
\begin{problem}[4] [20 points]
As we discussed in the lecture, the PCA algorithm is sensitive to scaling and pre-processing of data. In this problem, we explore few data pre-processing operations on data matrices.

\begin{enumerate}
    \item Explain what does each of the following data processing operation mean, why it is important, and how you apply to a data matrix $\bm{X} \in \mathbb{R}^{n \times d}$, $n$ samples each with $d$ features \footnote{For an implementation of these operation in \textsf{scikit-learn} please see \href{https://scikit-learn.org/stable/modules/preprocessing.html}{this}.} (if it helps, use mathematical equations to show the calculations).
    \begin{enumerate}
        \item Centering or mean removal
        \item Scaling of a feature to a range $[a, b]$
        \item Standardization
        \item Normalization (between 0 and 1)
    \end{enumerate}
    \item Apply the above operations to the following data matrix with 5 samples and two features independently and show the processed data matrix. You will have 4 different matrices based on each processing operation. For scaling pick $a = 0$ and $b = 1$.
        \begin{equation*}
            \bm{X} = \begin{bmatrix}
            1 & 2\\
            -1 & 1\\
            0 & 1\\
            2 & 4\\
            3 & 1\\
            \end{bmatrix}
        \end{equation*}
\end{enumerate}
\end{problem}

% \begin{figure}[ht!]
% \centering
%   \includegraphics[width=0.5\textwidth]{mdp.png}
%   \caption{A MDP with states $\mathcal{S}=\{s_0,s_1,s_2,s_3\}$ and actions $\mathcal{A}=\{a_0,a_1,a_2\}$. The reward is 10 for state $s_3$, 1 for state $s_2$, and 0 otherwise.}
%   \label{fig:mdp}
% \end{figure}

% \begin{problem}[5] [20 points] Consider the MDP represented by a graph in Figure~\ref{fig:mdp} with discount factor $\gamma \in [0, 1)$. States are represented by circles. The pair on the arrows shows the action to be taken and the transition probability from a state to another state, respectively (please note that the representation of MDP as a graph here is slightly different from Example 3.3 in the book). Each of the parameters $p$ and $q$ are in the interval $[0, 1]$. The reward is 10 for state $s_3$, 1 for state $s_2$, and 0 otherwise. Note that this means, from $s_0$, after taking an action, it will receive 0 reward no matter what the action is. This is similar for other states. 
% \begin{enumerate}
%     \item List all two deterministic policies (for each state, choose an action with probability 1) for MDP in Figure~\ref{fig:mdp} starting from the initial state $s_0$ and ending at state $s_3$. For each policy $\pi$ compute the value of each state, $v_{\pi}(s_0)$, $v_{\pi}(s_1)$, $v_{\pi}(s_2)$, and $v_{\pi}(s_3)$ for $\gamma = 0$ and $p = q = \frac{1}{2}$.
%     \item Show the equation representing the optimal value function for each state, i.e. $v_{\ast}(s_0)$, $v_{\ast}(s_1)$, $v_{\ast}(s_2)$, and $v_{\ast}(s_3)$. Please note the value of states will be a function of parameters $p$, $q$, and $\gamma$.
%     % \item Is there a value for $p$ such that for all $\gamma \in [0,1)$ and $q \in [0,1]$, $\pi_{\ast}(s_0)=a_2$?
%     \item Using $p = q = 0.25$ and $\gamma = 0.9$, compute $\pi_{\ast}$ and $v_{\ast}$ for all states of MDP. You can either solve the recursion of part (2) or implement value iteration (if you intend to do the latter, consider the approximation error $\epsilon = 10^{-3}$ as stopping criteria, and attache your code in your submission).
% \end{enumerate}

% \end{problem}

% \begin{problem}[6] [10 points]
% As discussed in the lecture, a key difference between Sarsa and Q-learning is that Sarsa is on-policy, while Q-learning is off-policy.
% Now, Suppose action selection is greedy in Sarsa and Q-learning. Is Q-learning then exactly the same algorithm as Sarsa? Will they make exactly the same action selections and weight updates?
% The pseudocode of both algorithms are shown below.
% \begin{figure}[h!]
% \centering
%   \includegraphics[width=0.7\textwidth]{images/sarsa_qlearning.png}
%   \caption{The pseudocode of SARSA and Q-Learning.}
% \end{figure}
% \end{problem}

\end{document}