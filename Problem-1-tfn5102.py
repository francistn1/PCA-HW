from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import preprocessing
import numpy as np
from sklearn.decomposition import PCA

iris = load_iris()
X = iris.data
y = iris.target
x = np.array(X)

pca1 = PCA(n_components=1)
pca1.fit(x)

# (1) Carry out a principal component analysis on the entire raw dataset (4-dimensional instances)
# for k = 1, 2, 3, 4 components
print("\n n_components = 1 \n")
print(" Explained Variance Ratio: ", pca1.explained_variance_ratio_)
print(" Explained Variance: ", pca1.explained_variance_)

pca2 = PCA(n_components=2)
pca2.fit(x)

print("\n n_components = 2 \n")
print(" Explained Variance Ratio: ", pca2.explained_variance_ratio_)
print(" Explained Variance: ", pca2.explained_variance_)

pca3 = PCA(n_components=3)
pca3.fit(x)

print("\n n_components = 3 \n")
print(" Explained Variance Ratio: ", pca3.explained_variance_ratio_)
print(" Explained Variance: ", pca3.explained_variance_)

pca4 = PCA(n_components=4)
pca4.fit(x)

print("\n n_components = 4 \n")
print(" Explained Variance Ratio: ", pca4.explained_variance_ratio_)
print(" Explained Variance: ", pca4.explained_variance_)

print("End of part 1")

# (2) Apply the centering and standardization operations from Problem 1 on raw data set and
# repeat part (1) on processed data. Explain any differences you observe compared to part (1)
# and justify.

x_scaled = preprocessing.scale(x)
x_min_max = preprocessing.MinMaxScaler().fit_transform(x_scaled)
x_normalized = preprocessing.normalize(x_min_max, norm='l2')


print("\n Processed Data, n_components = 1 \n")
pca11 = PCA(n_components=1)
pca11.fit(x_normalized)
print(" Explained Variance Ratio: ", pca11.explained_variance_ratio_)
print(" Explained Variance: ", pca11.explained_variance_)

print("\n Processed Data, n_components = 2 \n")
pca12 = PCA(n_components=2)
pca12.fit(x_normalized)
print(" Explained Variance Ratio: ", pca12.explained_variance_ratio_)
print(" Explained Variance: ", pca12.explained_variance_)

print("\n Processed Data, n_components = 3 \n")
pca13 = PCA(n_components=3)
pca13.fit(x_normalized)
print(" Explained Variance Ratio: ", pca13.explained_variance_ratio_)
print(" Explained Variance: ", pca13.explained_variance_)

print("\n Processed Data, n_components = 4 \n")
pca14 = PCA(n_components=4)
pca14.fit(x_normalized)
print(" Explained Variance Ratio: ", pca14.explained_variance_ratio_)
print(" Explained Variance: ", pca14.explained_variance_)
print("End of part 2")


# (3) Project the raw four dimensional data down to a two-dimensional subspace generated by first
# two top principal components (PCs) from part (2) and produce a scatter plot of the data.
# Make sure you plot each of the three classes differently (using color or different markers).
# Can you see the three Iris flower clusters?


pca12 = PCA(n_components=2)
a = pca12.fit_transform(x_normalized)
print(" Explained Variance Ratio: ", pca12.explained_variance_ratio_)
print(" Explained Variance: ", pca12.explained_variance_)

plt.scatter(a[:, 0], a[:, 1], c=y, edgecolor='k')
plt.show()
print("End of part 3")

# Either use your k-means++ implementation from previous homework or from scikit-learn to
# cluster data from part (3) into three clusters. Explain your observations
kMeans = KMeans(n_clusters=3, init='k-means++')
k = kMeans.fit(a)
plt.scatter(a[:, 0], a[:, 1], c=k.labels_, edgecolors='k')
plt.show()
